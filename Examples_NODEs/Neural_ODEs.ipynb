{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510b9ee2-5942-4380-97ab-fc03d905cde6",
   "metadata": {},
   "source": [
    "# Neural Ordinary Differential Equations (DiffEqFlux tutorial)\n",
    "https://docs.sciml.ai/DiffEqFlux/stable/examples/neural_ode/\n",
    "\n",
    "A neural ODE is an ODE where a neural network defines its derivative function. For example, with the multilayer perceptron neural network Lux.Chain(Lux.Dense(2, 50, tanh), Lux.Dense(50, 2)), we can define a differential equation which is u' = NN(u). This is done simply by the NeuralODE struct. Let's take a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3148c1-5013-45ed-b882-a81eff41b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ComponentArrays, Lux, DiffEqFlux, OrdinaryDiffEq, Optimization, OptimizationOptimJL\n",
    "using OptimizationOptimisers, Random, Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c863d-4ab2-4e19-bdeb-c60a5965f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "using IRKGaussLegendre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91277da4-eeff-4060-b709-6dc1cbad22e5",
   "metadata": {},
   "source": [
    "Let's get a time series array from a spiral ODE to train against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84170117-0dab-41c6-a081-193f05f3968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = Xoshiro(0)\n",
    "u0 = Float32[2.0; 0.0]\n",
    "datasize = 30\n",
    "tspan = (0.0f0, 1.5f0)\n",
    "tsteps = range(tspan[1], tspan[2]; length = datasize)\n",
    "\n",
    "function trueODEfunc(du, u, p, t)\n",
    "    true_A = [-0.1 2.0; -2.0 -0.1]\n",
    "    du .= ((u .^ 3)'true_A)'\n",
    "end\n",
    "\n",
    "prob_trueode = ODEProblem(trueODEfunc, u0, tspan)\n",
    "#ode_data = Array(solve(prob_trueode, Tsit5(); saveat = tsteps))\n",
    "ode_data = Array(solve(prob_trueode, IRKGL16(); dt=0.25, saveat = tsteps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa1bed-8bad-4875-9462-ee69e8323fa3",
   "metadata": {},
   "source": [
    "Now let's define a neural network with a NeuralODE layer. First, we define the layer. Here we're going to use Lux.Chain, which is a suitable neural network structure for NeuralODEs with separate handling of state variables:\n",
    "\n",
    "In our model, we used the x -> x.^3 assumption in the model. By incorporating structure into our equations, we can reduce the required size and training time for the neural network, but a good guess needs to be known!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3ec1ee-e1ed-426c-adb0-db50f77306dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dudt2 = Chain(x -> x .^ 3, Dense(2, 50, tanh), Dense(50, 2))\n",
    "p, st = Lux.setup(rng, dudt2)\n",
    "#prob_neuralode = NeuralODE(dudt2, tspan, Tsit5(); saveat = tsteps)\n",
    "prob_neuralode = NeuralODE(dudt2, tspan, IRKGL16(); dt=0.25, saveat = tsteps, sensealg = BacksolveAdjoint())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5238402-e1ce-475a-ad80-cbcf870a4d12",
   "metadata": {},
   "source": [
    "From here we build a loss function around it. The NeuralODE has an optional second argument for new parameters, which we will use to change the neural network iteratively in our training loop. We will use the L2 loss of the network's output against the time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd769b-c98b-4ad4-9e93-a8badead304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict_neuralode(p)\n",
    "    Array(prob_neuralode(u0, p, st)[1])\n",
    "end\n",
    "\n",
    "function loss_neuralode(p)\n",
    "    pred = predict_neuralode(p)\n",
    "    loss = sum(abs2, ode_data .- pred)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3958e0-5ed6-4fa6-bde4-7ffebf70871b",
   "metadata": {},
   "source": [
    "We define a callback function. In this example, we set doplot=false because otherwise it would show every step and overflow the documentation, but for your use case set doplot=true to see a live animation of the training process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0aa6c-ff8c-4754-b9f6-d690bbca8407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not plot by default for the documentation\n",
    "# Users should change doplot=true to see the plots callbacks\n",
    "function callback(state, l; doplot = false)\n",
    "    println(l)\n",
    "    # plot current prediction against data\n",
    "    if doplot\n",
    "        pred = predict_neuralode(state.u)\n",
    "        plt = scatter(tsteps, ode_data[1, :]; label = \"data\")\n",
    "        scatter!(plt, tsteps, pred[1, :]; label = \"prediction\")\n",
    "        display(plot(plt))\n",
    "    end\n",
    "    return false\n",
    "end\n",
    "\n",
    "pinit = ComponentArray(p)\n",
    "callback((; u = pinit), loss_neuralode(pinit); doplot = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c084d5-1866-4142-8808-ff3c100c2b18",
   "metadata": {},
   "source": [
    "We then train the neural network to learn the ODE.\n",
    "\n",
    "Here we showcase starting the optimization with Adam to more quickly find a minimum, and then honing in on the minimum by using LBFGS. By using the two together, we can fit the neural ODE in 9 seconds! (Note, the timing commented out the plotting). You can easily incorporate the procedure below to set up custom optimization problems. For more information on the usage of Optimization.jl, please consult this documentation.\n",
    "\n",
    "The x and p variables in the optimization function are different from x and p above. The optimization function runs over the space of parameters of the original problem, so x_optimization == p_original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ac701-b22d-4c6a-aa0f-01467bc32a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Optimization.jl to solve the problem\n",
    "adtype = Optimization.AutoZygote()\n",
    "\n",
    "optf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\n",
    "optprob = Optimization.OptimizationProblem(optf, pinit)\n",
    "\n",
    "result_neuralode = Optimization.solve(\n",
    "    optprob, OptimizationOptimisers.Adam(0.05); callback = callback, maxiters = 300)\n",
    "\n",
    "callback((; u = result_neuralode.u), loss_neuralode(result_neuralode.u); doplot = true)\n",
    "\n",
    "optprob2 = remake(optprob; u0 = result_neuralode.u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb0e49-7854-4970-ad75-a77811b22495",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback((; u = result_neuralode.u), loss_neuralode(result_neuralode.u); doplot = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1808f88a-ebd5-43ec-8eaa-1e2029ac904e",
   "metadata": {},
   "source": [
    "We then complete the training using a different optimizer, starting from where Adam stopped. We do allow_f_increases=false to make the optimization automatically halt when near the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c7641-1149-46a8-98b9-0274220b0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_neuralode2 = Optimization.solve(\n",
    "    optprob2, Optim.BFGS(; initial_stepnorm = 0.01); callback, allow_f_increases = false)\n",
    "\n",
    "callback((; u = result_neuralode2.u), loss_neuralode(result_neuralode2.u); doplot = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de92c0-c2b7-45e5-902a-a01f28d65bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
